# EvalAgent Configuration Template
# Copy this file to config.yaml and fill in your API keys

# LLM API Configuration
llm_apis:
  openai:
    api_key: "your_openai_api_key_here"
    base_url: "https://api.openai.com/v1"
    models:
      - "gpt-3.5-turbo"
      - "gpt-4"
      - "gpt-4-turbo"
  
  anthropic:
    api_key: "your_anthropic_api_key_here"
    base_url: "https://api.anthropic.com"
    models:
      - "claude-3-haiku-20240307"
      - "claude-3-sonnet-20240229"
      - "claude-3-opus-20240229"
  
  google:
    api_key: "your_google_api_key_here"
    models:
      - "gemini-pro"
      - "gemini-ultra"
  
  cohere:
    api_key: "your_cohere_api_key_here"
    models:
      - "command-r"
      - "command-r-plus"
  
  ollama:
    api_key: "not_required"  # Ollama doesn't require API keys
    base_url: "http://localhost:11434"
    models:
      - "llama3:8b"
      - "llama3:70b"
      - "codellama:7b"
      - "codellama:13b"
      - "codellama:34b"
      - "mistral:7b"
      - "mixtral:8x7b"
      - "vicuna:7b"
      - "wizard-vicuna-uncensored:7b"
      - "orca-mini:3b"

# Evaluation Configuration
evaluation:
  timeout: 30  # seconds
  max_retries: 3
  temperature: 0.1
  max_tokens: 2048
  
  # Pass@k configuration
  pass_at_k:
    k_values: [1, 5, 10]
    num_samples: 10

# Benchmark Configuration
benchmarks:
  humaneval:
    enabled: true
    path: "benchmarks/humaneval"
    timeout: 10
  
  mbpp:
    enabled: true
    path: "benchmarks/mbpp"
    timeout: 15
  
  custom:
    enabled: false
    path: "benchmarks/custom"
    timeout: 20

# Output Configuration
output:
  format: ["json", "csv", "html"]
  detailed_logs: true
  save_generated_code: true
  
# Logging Configuration
logging:
  level: "INFO"
  file: "logs/evalagent.log"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"